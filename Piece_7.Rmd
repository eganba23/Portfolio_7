---
title: "Piece 7 - Evaluation of Lab 09B: Algorithmic Bias"
author: "Benjamin Egan"
date: "4/1/25"
output: github_document
---

# Evaluation and tips for Lab 9

This assignment is a new lab, and is in the development stage. This piece is designed to be my commentary on the lab, providing my thoughts about questions, what I was confident in doing, the direction I took in my answers, places where I became confused, etc. 

Here is the link to the assignment: https://datascience4psych.github.io/DataScience4Psych/lab09.html. You use this to view the lab in it's current form.

For each question I will include the question in bold, my answer (#A), and commentary. I include the word commentary so that you, the reader, can quickly search to get my commentary. 

```{r load-packages, message = FALSE, warning = FALSE, echo = FALSE}
library(tidyverse)
library(fairness)
library(janitor)
```

### The data

Cleaning up some of the names
```{r load dataset}
compas <- read_csv("data/compas-scores-2-years.csv") %>% 
  clean_names() %>% 
  rename(decile_score = decile_score_12,
                         priors_count = priors_count_15)
view(compas)

nrow(compas)
ncol(compas)
```


## Part 1 - Exploring the Data

#### 1. What are the dimensions of the COMPAS dataset? (Hint: Use inline R code and functions like nrow and ncol to compose your answer.) What does each row in the dataset represent? What are the variables?

1A. Each row of the dataset represents an individual who arrested in Broward County, Florida. There are 7,214 people in the dataset, and 53 different variables

#### 2. How many unique defendants are in the dataset? Is this the same as the number of rows? If not, why might there be a difference?

2A. There should be 7,214 unique people in the dataset. After a quick glance through the dataset using view(), I haven't seen a repeated ID number or person's name.

### Visualizing demographic data

##### 3. Let’s examine the distribution of the COMPAS risk scores (decile_score)! What do you observe about the shape of this distribution?

```{r basic distribution}

compas %>%
  ggplot(aes(
    x = decile_score
  ))+
  geom_histogram(stat = "count")+
  labs(
    x = "risk score from 1-10",
    y = NULL,
    title = "Distrubtion of COMPAS Scores",
    subtitle = "higher scores mean greater risk"
  )

```

3A. Skewed right, indicating there are more poeple with lower risk.

#### 4. Let’s examine the demographic distribution in our dataset. Create visualizations to show by race, sex, and category

#### The distribution of defendants by race
```{r graph race, warning = FALSE}
Race_graph <- compas %>%
  ggplot(aes(
    x = race
  ))+
  geom_histogram(stat = "count", fill = "orange", alpha = .7)+
  theme_bw()+
  geom_text(
    stat = "count",
    aes(x = race,
    y = ..count..,
    label = ..count..),
    size = 4, 
    color = "black",
    vjust = -0.2) +
  labs(
    x = "Race",
    y = NULL,
    title = "Distribution of Defendents by Race",
    subtitle = "People arrested in Broward County, Florida"
  )

Race_graph

```

#### The distribution of defendants by sex
```{r graph sex, warning = FALSE}
Sex_graph <- compas %>%
  ggplot(aes(
    x = sex
  ))+
  geom_histogram(stat = "count", fill = c("pink", "steelblue2"))+
  theme_bw()+
  geom_text(
    stat = "count",
    aes(x = sex,
    y = (..count../2),
    label = ..count..),
    size = 7, 
    color = "black") +
  labs(
    x = "Sex",
    y = NULL,
    title = "Distribution of Defendents by Sex",
    subtitle = "People arrested in Broward County, Florida"
  )

Sex_graph

```

#### The distribution of defendants by age
```{r graph age, warning = FALSE}
Age_graph <- compas %>%
  ggplot(aes(
    x = age_cat
  ))+
  geom_histogram(stat = "count", fill = "gray", alpha = .7)+
  theme_bw()+
  geom_text(
    stat = "count",
    aes(x = age_cat,
    y = ..count..,
    label = ..count..),
    size = 5, 
    color = "black",
    vjust = -0.2) +
  labs(
    x = "Age",
    y = NULL,
    title = "Distribution of Defendents by Age",
    subtitle = "People arrested in Broward County, Florida"
  )

Age_graph

```


Part 1 Commentary: So far this has been relatively basic, and nothing we haven't already done thus far.

#### 4. For an extra challenge, try to create a single visualization that shows all three distributions side by side. You can use facets or color to differentiate between the different demographic groups.

Here I'm using plot_grid() from the cowplot package as one example of how to do this. There are clear issues (such as size of text), but it will get the job done.

```{r all graphs together}
library(cowplot)

plot_grid(Race_graph,Sex_graph, Age_graph)


```

I turned to chatGPT to see if it had any suggestions on fixing this issue. It initially recommended adding the arguments ncol = 1, align = "v" to the plot_grid(). This ended up creating this... unique outcome.

![This reminds me of Stretch Armstrong](chat_suggestion.png)

I told it I wanted the Axes to be fixed, and it gave me a great suggestion.

I can take the original dataset and create a subset of the data that just includes race, sex, and age. From there, I can use the wide-to-long transformation, making three separate rows for each person (one for race, one for sex, one for age). Category is the designation of the IV, and Value is what they identify as.

```{r demographic secondary dataset}
compas_long <- compas %>%
  pivot_longer(cols = c(race, sex, age_cat), names_to = "Category", values_to = "Value")
#view(compas_long)

```

Now, I can use facet_wrap() using category. This way, I can create three separate graphs side by side. I copied its suggestion and then made edits. These were for style and for aesthetic.

```{r optional demographic 2}

compas_long %>%
ggplot(aes(
  x = Value)) +
  geom_bar(aes(fill = Category), alpha = 0.7) +
  geom_text(
    stat = "count",
    aes(y = ..count.., 
        label = ..count..),
    vjust = -0.2,
    size = 3
  ) +
  theme_bw() +
  facet_wrap(~Category, labeller = as_labeller(c(`age_cat` = "Age", `race` = "Race", `sex` = "Sex")), scales = "free_x")+ # scales = "free_x" keeps different x-scales while aligning axes properly
  labs(
    x = NULL,
    y = "Count",
    title = "Distribution of Defendants",
    subtitle = "People arrested in Broward County, Florida"
  ) +
  theme(legend.position = "none",
        axis.text.x = element_text(angle = -45, vjust = 0.5, hjust=.5)
        )

```

Chat helped me figure out how to change all the colors to reflect the original colors I used. This was a direct copy paste, with me changing the colors.

```{r optional demographic with color}

compas_long %>% 
  ggplot(aes(x = Value, fill = Value)) +   
  geom_bar(alpha = 0.7) +   
  geom_text(     
    stat = "count",     
    aes(y = ..count.., label = ..count..),     
    vjust = -0.2,     
    size = 3   
  ) +   
  theme_bw() +   
  facet_wrap(~Category, 
             labeller = as_labeller(c(`age_cat` = "Age", 
                                      `race` = "Race", 
                                      `sex` = "Sex")), 
             scales = "free_x") +  
  labs(     
    x = NULL,     
    y = "Count",     
    title = "Distribution of Defendants",     
    subtitle = "People arrested in Broward County, Florida"   
  ) +   
  theme(
    legend.position = "none",
    axis.text.x = element_text(angle = -45, vjust = 0.5, hjust = .5)
  ) + 
  scale_fill_manual(
    values = c("25 - 45" = "gray", 
               "Greater than 45" = "gray", 
               "Less than 25" = "gray",
               "African-American" = "orange", 
               "Caucasian" = "orange", 
               "Asian" = "orange", 
               "Hispanic" = "orange", 
               "Native American" = "orange", 
               "Other" = "orange",
               "Male" = "royalblue", 
               "Female" = "hotpink")
  )

```

4 Commentary: This was certainly harder than I thought. I included my thoughts as I went along. Some of these answers (like using cowplot package) I got from googling, and others I got from ChatGPT. I think this was a cool additional exercise that made me think outside the box and I got learn about Labeller() in facet_wrap().

#### Visualization of the COMPAS risk scores

```{r COMPAS scores visual, warning = FALSE}

compas %>%
  ggplot(aes(
    x = decile_score
  ))+
  theme_bw()+
  geom_histogram(stat = "count", fill = "gray")+
    geom_text(
    stat = "count",
    aes(x = decile_score,
    y = ..count..,
    label = ..count..),
    size = 3.5, 
    color = "black",
    vjust = -0.2) +
  labs(
    x = "COMPAS risk scores",
    y = NULL,
    title = "Distribution of Defendents by COMPAS risk",
    subtitle = "People arrested in Broward County, Florida"
  )

```

## Part 2 - Risk scores and recidivism

####  5. Create a visualization showing the relationship between risk scores (decile_score) and actual recidivism (two_year_recid). Do higher risk scores actually correspond to higher rates of recidivism?



```{r risk score to recidivism visual}

compas %>%
  ggplot(aes(
    x = decile_score
  ))+
  facet_wrap(~two_year_recid, labeller = as_labeller(c(`0` = "Did not recidivate within two years", `1` = "Recidivated within two years")))+
  geom_histogram()+
  theme_bw()+
   labs(
    x = "Whether the defendant recidivated within two years (0 = no, 1 = yes)",
    y = "COMPAS risk score",
    title = "The relationship between risk scores and actual recidivism"
  )

compas %>%
  ggplot(aes(
    x = two_year_recid,
    y = decile_score
  ))+
  geom_smooth(formula = y~x, color = "black")+
  theme_bw()+
   labs(
    x = "Whether the defendant recidivated within two years (0 = no, 1 = yes)",
    y = "COMPAS risk score",
    title = "The relationship between risk scores and actual recidivism"
  )

```

5A. Assuming I did this correctly, it appears that recidivism, on average, increases COMPAS risk score.

5 Commentary: I wasn't completely clear on what your vision for this question may have been. The geom_smooth() appeared to be showing the relationship and suggesting that people who recidivate have higher scores. But the histogram showed otherwise. I would recommend having students try both out and comment on what the two would show. Or provide them with the geom_smooth() and say "this might not be accurate"


#### Alternate plot
```{r alt plot}

compas %>%
  ggplot(aes(
    x = decile_score
  ))+
  geom_histogram()+
  geom_text(
    aes(x = decile_score,
        y = ..count..,
        label = ..count..),
    stat = "count",
    size = 3.5, 
    color = "black",
    vjust = -0.2) +
  facet_wrap(~ two_year_recid, 
             labeller = as_labeller(c(`0` = "Recidivated within two years", `1` = "Did not recidivate within two 
                                      years")))+
  labs(
    x = "COMPAS risk score from 1-10 (higher = greater risk)",
    y = NULL,
    title = "The relationship between risk scores and actual recidivism"
  )

```

This plot is more showing the breakdown of COMPAS scores based on recidivism.


### COMPAS Accuracy

#### 6. Calculate the overall accuracy of the COMPAS algorithm. For this exercise, consider a prediction “correct” if:
#### A defendant with a high risk score (decile_score >= 7) did recidivate (two_year_recid = 1)
#### A defendant with a low risk score (decile_score <= 4) did not recidivate (two_year_recid = 0)

```{r compas accuracy, warning = FALSE}

compas_accuracy <- compas %>%
  mutate(compas_accurate = case_when(
    decile_score >= 7 & two_year_recid == 1 ~ "Accurate, committed crime",
    decile_score < 7 & two_year_recid == 1 ~ "Not acccurate, committed crime",
    decile_score <= 4 & two_year_recid == 0 ~ "Accurate, no further crime",
    decile_score > 4 & two_year_recid == 0 ~ "Not accurate, no further crime"
  ))

percent_accurate <- compas %>%
  mutate(compas_accurate = case_when(
    decile_score >= 7 & two_year_recid == 1 ~ 1,
    decile_score < 7 & two_year_recid == 1 ~ 0,
    decile_score <= 4 & two_year_recid == 0 ~ 1,
    decile_score > 4 & two_year_recid == 0 ~ 0
  ))

compas_accuracy %>%
  ggplot(aes(
    x = compas_accurate,
    fill = compas_accurate
  ))+
  geom_histogram(stat="count")+
  scale_fill_manual(values=c("green4", "green4","red2", "red2"))+
  coord_flip()+
  labs(
    x = "Accuracy of COMPAS",
    y = NULL,
    title = "How accurate is the COMPAS based on recidivism"
  )+
  theme(legend.position = "none")

```

6A. It's hard to tell. Yes there are lots of people who it accurately predicts, but that could be based on how people score on the COMPAS/their recidivism.

6 Commentary: This took me a minute to figure out. My understanding for this question is that what you provided me was "correct" and everything else was "incorrect".


#### 7. How well does the COMPAS algorithm perform overall? What percentage of its predictions are correct based on your calculation above?

```{r accuracy calcuation}

committed_crime_test <- compas %>%
  mutate(compas_accurate = case_when(
    decile_score >= 7 & two_year_recid == 1 ~ "Accurate, committed crime",
    decile_score < 7 & two_year_recid == 1 ~ "Not acccurate, committed crime",
    decile_score <= 4 & two_year_recid == 0 ~ "Accurate, no further crime",
    decile_score > 4 & two_year_recid == 0 ~ "Not accurate, no further crime"
  ))

df_percent1 <- count(percent_accurate, compas_accurate)

Rec_total_accurate <- df_percent1 %>%
  filter(compas_accurate == 0) %>% select(n)

Rec_total_innacute <- df_percent1 %>%
  filter(compas_accurate == 1) %>% select(n)

Rec_total <- df_percent1 %>% summarize(sum = sum(n))

Rec_total_accurate/Rec_total*100

committed_crime <- compas %>%
  mutate(compas_accurate = case_when(
    decile_score >= 7 & two_year_recid == 1 ~ 1,
    decile_score < 7 & two_year_recid == 1 ~ 2,
    decile_score <= 4 & two_year_recid == 0 ~ 3,
    decile_score > 4 & two_year_recid == 0 ~ 4
  ))


df_percent2 <- count(committed_crime, compas_accurate)

acc_commit <- df_percent2 %>%
  filter(compas_accurate == 1) %>% select(n)

not_acc_commit <- df_percent2 %>%
  filter(compas_accurate == 2) %>% select(n)

acc_no_commit <- df_percent2 %>%
  filter(compas_accurate == 3) %>% select(n)

not_acc_no_commit <- df_percent2 %>%
  filter(compas_accurate == 4) %>% select(n)

total_breakdown <- df_percent2 %>% summarize(sum = sum(n))

acc_commit/total_breakdown*100

acc_commit/(acc_commit + not_acc_commit)*100

acc_no_commit/(acc_no_commit+not_acc_no_commit)*100


```


7A. The COMPAS algorithm doesn't perform great (44% overall accuracy, 41% accurate among people who commit a crime). It's mostly accurate for predicting people who won't commit another crime (67%).

7 Commentary: This seemed so silly, but I struggled figuring out what "accuracy" meant. More specifically, what was the denominator for each of the accuracy percentages I made. I think the arbitrary nature of "How well does the COMPAS algorithm perform overall" led me to over think the answers.

## Part 3 - Investigating Disparities


#### 8. Create visualizations comparing the distribution of risk scores (decile_score) between Black and white defendants. Do you observe any differences?

#### 9. Calculate the percentage of Black defendants and white defendants who were classified as high risk (decile_score >= 7). Is there a disparity?

```{r race disparity}

race_graph <- compas %>%
filter(race == c("Caucasian","African-American"))


COMPAS_race_graph <- race_graph %>%
  ggplot(aes(
    x = decile_score
  ))+
  facet_wrap(~ race, labeller = as_labeller(c(`Caucasian` = "White", `African-American` = "Black")))+
  geom_histogram(stat = "count")+
  labs(
    x = "COMPAS risk score from 1-10 (higher = greater risk)",
    y = NULL,
    title = "Race disparities on the COMPAS scores"
  )

COMPAS_race_graph

race_accurate <- race_graph %>%
  mutate(race_compas_accurate = case_when(
    decile_score >= 7 & race == "Caucasian" ~ 1,
    decile_score >= 7 & race == "African-American" ~ 0
  ))


race_percent <- count(race_accurate, race_compas_accurate)



White_accurate <- race_percent %>%
  filter(race_compas_accurate == 1) %>% select(n)

Black_accurate <- race_percent %>%
  filter(race_compas_accurate == 0) %>% select(n)

Black_accurate/(White_accurate + Black_accurate )*100


```

8 & 9A. There's a clear difference in distribution between white and black defendants who were classified as high risk, as 78% of people who had a COMPAS score above 7 were black.

8 and 9 Commentary: I used the same method I did earlier, just filtering for race. It wasn't easy, but also assumed I did the process correct up above. Might want scaffolding here to make sure people are missing the point twice.

### 10. Now, let’s look at the accuracy of predictions for different racial groups. Calculate the following metrics separately for Black defendants and white defendants:

#### False Positive

10 Commentary: I'll admit I tried this 30 different ways. I turned to ChatGPT to ask for help. We went back and forth on code that didn't initially work, and I teased out what parts weren't working and what I truly needed. We settled on this code.
```{r false positive accuracy}

non_recidivists <- race_graph %>%
  filter(two_year_recid == 0)


total_non_recid <- non_recidivists %>%
  group_by(race) %>%
  summarise(total_non_recid = n())


false_positives <- non_recidivists %>%
  filter(decile_score >= 7) %>%
  group_by(race) %>%
  summarise(false_positives = n())


fpr_results <- left_join(false_positives, total_non_recid, by = "race") %>%
  mutate(FPR = false_positives / total_non_recid * 100)

fpr_results


```

#### False Negative

I edited the code from above.

```{r false negative accuracy}

recidivists <- race_graph %>%
  filter(two_year_recid == 1)


total_recid <- recidivists %>%
  group_by(race) %>%
  summarise(total_recid = n())


false_negatives <- recidivists %>%
  filter(decile_score <= 4) %>%
  group_by(race) %>%
  summarise(false_negatives = n())


fnr_results <- left_join(false_negatives, total_recid, by = "race") %>%
  mutate(FNR = false_negatives / total_recid * 100)

fnr_results

```

#### Visualizing False positives and negatives

```{r visualize FP and FN}

error_rates1 <- merge(fnr_results,fpr_results, by="race")

error_rate_adjusted <- error_rates1 %>%
  pivot_longer(cols = -c(race, total_recid,), names_to = "Error_type")

error_rate_adjusted %>%
  ggplot(aes(
    x = race, 
    y = value, 
    group = Error_type,
    color = Error_type
  )) +
  geom_line(size = 1) +
  geom_point(size = 3) +
  facet_wrap(~Error_type, scales = "free", 
      labeller = as_labeller(c(
         `false_negatives` = "False Negatives", 
         `false_positives` = "False Positives", 
         `FNR` = "False Negative Proportion",
         `FPR` = "False Positive Proportion", 
         `total_non_recid` = "Non-recidivism"
             ))) +
  scale_color_manual(values = c(
    "false_negatives" = "red",
    "false_positives" = "blue",
    "FNR" = "darkgreen",
    "FPR" = "purple",
    "total_non_recid" = "orange"
  )) +
  theme_bw()+
  theme(axis.text.x = element_text(angle = -10, vjust = 0.5, hjust=.5))+
  labs(
    x = "Race",
    y = NULL,
    title = "Accuracy of predictions for different racial groups",
    color = "Prediction Results"
  )

```

#### 11. Create a visualization comparing these metrics between Black and white defendants. What disparities do you observe?

11A. Based on the above, we can see that Black defendants have higher false positives. This means black defendants are more likely to be non-recidivists who were classified as high risk. Although there were more false negatives (recidivists who were classified as low risk), this can be explained by the false negative proportion. There were twice as many black defendants classified as recidivists than white defendants.

11 Commentary: I understood that questions 10 and 11 were linked. I chatted with you about this answer. I think I did this correctly, but I'll admit I didn't fully understand what I was supposed to draw from this information until our discussion. I would add in scaffolding that has them look at the numbers (both the numbers and the proportions). By doing this as I was talking with you, I then better understood the reason for looking at false positive and false negative rates and proportion.

## Part 4 - Understanding the source bias

#### 12. Let’s investigate what factors might be contributing to the disparities we’ve observed. Create a visualization showing the relationship between prior convictions (priors_count) and risk score (decile_score), colored by race. Does the algorithm weigh prior convictions differently for different racial groups?

```{r prior and risk score visual, fig.height=8, fig.width=10}

compas %>%
  ggplot(aes(
    x = decile_score,
    y = priors_count,
    color = race
  ))+
  geom_smooth()+
  geom_jitter(width = .1, alpha = .6)+
  theme_bw()

```

This is really hard to see. You can kind of tell there's more green (Caucasian) lower on the risk score and more red (African American) higher up. I filtered to only look at Caucasian and African American individuals and put them side by side. 

```{r other prior and risk score visual}

race_graph %>%
  ggplot(aes(
    x = decile_score,
    y = priors_count
  ))+
  geom_smooth()+
  geom_jitter(width = .15, alpha = .6)+
  facet_wrap(~race)+
  theme_bw()+
  labs(
    x = "Risk score",
    y = "Number of prior offenses",
    title = "Race discrepancies for the number of convictions"
  )

```

Here you can tell there are way less white defendants higher up on risk scale, and that African Americans have more prior convictions. It looks like the algorithm doesn't weigh convictions differently? I'm actually not quite sure.

13. [I'll start by saying I didn't exactly understand the question] I think there is evidence supporting ProPublica’s claim. There is a higher amount of false positives for Black defendants compared to white defendants. Additionally, the accuracy is poor (44%), suggesting that focusing on calibration might lead to errors.

## Part 5 - Designing a more fair algorithm

14. One of the first things I would look at is the the charge types. The current algorithm does not take into account the type of crimes committed. I am thinking more violent crimes would lead to a better measure of likelihood of committing another crime. I am also thinking that juvenile offenses could be a good predictor, as people with more juvenile offenses and more felonies (compared to misdemeanors) would indicate future crime.

15. A fair algorithm would probably have to weigh human bias. This could mean understanding and factoring in racism, sexism, etc. For example, black people might be more likely to have a false positive arrest (i.e. arrested while innocent). To some it might look mathematically wrong to weight black defendants differently compared to white defendants, but this might increases our accuracy.

16. Algorithms should be uses as a guide, not as a decision maker. Knowing the current accuracy of COMPAS is so low, a human needs to make the final decision as to an individual's status. Based on what I've seen, I would make it  policy that any evidence for creating policy that is supported by an algorithm need to be taken through a period of review. In this review, I would create similar analyses in this lab, and make sure that the algorithm is up to the same standard as scientific review (i.e. alpha .05 standard), ensuring that the algorithm is not due to chance.

# Stretch Goal

Note this will be a work in progress, as it is a stretch goal and I am still learning.

*17. I reran the visualization from above, showing prior convictions.

```{r stretch other prior and risk score visual}

race_graph %>%
  ggplot(aes(
    x = decile_score,
    y = priors_count
  ))+
  geom_smooth()+
  geom_jitter(width = .15, alpha = .6)+
  facet_wrap(~race)+
  theme_bw()+
  labs(
    x = "Risk score",
    y = "Number of prior offenses",
    title = "Race discrepancies for the number of convictions"
  )

```

As I explained above, the distribution for black defendants is more evenly spaced out compared to white defendants, which is more skewed left. It looks similar to the other graph, just looking at risk score.

```{r extra point}
COMPAS_race_graph
```

##### Question 19 - other variables of interest

```{r COMPAS and other variables}

race_graph %>%
  ggplot(aes(
    x = decile_score,
    fill = c_charge_degree
  ))+
  geom_histogram(stat = "count")+
  scale_fill_manual(labels = c("Felony", "Misdemeanor"), values = c("red3", "yellow3"))+
  facet_grid(c_charge_degree ~ race, labeller = as_labeller( c(
        'African-American' = "Black",
        'Caucasian' = "White",
        'F' = "Felony",
       'M' = "Misdemeanor"
        )))+
  theme_bw()+
  labs(
    x = "Risk score",
    y = "Number of prior offenses",
    title = "Race discrepancies for the number of convictions"
      )+
    theme(legend.position = "none")

```

Misdemeanors are generally your least severe charges. Felonies are murder, rape, breaking into cars, breaking into houses, they are more serious crimes. We can see there are a lot more felonies associated with black dependents than compared to white dependents. I would guess that the type of charge certainly helps lead to the disparities in risk score.

### Building a fairer algorithm

```{r start build}

recid_model <- glm(
  two_year_recid ~ age + priors_count + c_charge_degree,
  data = compas,
  family = binomial()
)


compas <- compas %>%
  mutate(
    predicted_prob = predict(recid_model, type = "response"),
    our_high_risk = predicted_prob >= 0.5
  )

```

